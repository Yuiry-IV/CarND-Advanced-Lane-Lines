{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>Advanced Lane Finding Project</h1>\n",
    "\n",
    "<b>by Y.Ivanov</b>\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a href=\"https://review.udacity.com/#!/rubrics/1966/view\">Rubric Points</a></h2>\n",
    "\n",
    "Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Writeup / README</h2>\n",
    "\n",
    "You're reading it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Camera Calibration</h2>\n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: \n",
    "\n",
    "Chess board can't be detected on images 1,4,5 and images 7 and 15 has been provded in wrong size:\n",
    "<code>\n",
    "findChessboardCorners ERROR on: ../camera_cal/calibration4.jpg\n",
    "../camera_cal/calibration7.jpg (721, 1281)\n",
    "findChessboardCorners ERROR on: ../camera_cal/calibration1.jpg\n",
    "findChessboardCorners ERROR on: ../camera_cal/calibration5.jpg\n",
    "../camera_cal/calibration15.jpg (721, 1281)\n",
    "</code>\n",
    "\n",
    "Here provided samples of original and undistoreed images \n",
    "<img src = \"../output_images/01_cam_cal_01.png\" alt=\"sample 01\"> \n",
    "<img src = \"../output_images/01_cam_cal_02.png\" alt=\"sample 02\"> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipeline (test images)</h2>\n",
    "<h3>1. A distortion-correcting </h3>\n",
    "Undistortion function has been caled at line 182:\n",
    "<code>\n",
    "        image = cv2.undistort( image, self.mtx, self.dist )\n",
    "</code>\n",
    "<img src=\"../output_images/02_pipeline_01.png\">\n",
    "\n",
    "<h3>2. A color transformation </h3>\n",
    "Performed by <b>hsv_select_v</b>\n",
    "<code>\n",
    "        image_sel = hsv_select_v( image, threshold=(self.v_treshold,255) )\n",
    "</code>\n",
    "\n",
    "<img src=\"../output_images/02_pipeline_02.png\">\n",
    "\n",
    "Threshold value depends from video and ligthning coditions and fine tuned manually.\n",
    "\n",
    "<h3>3. Ceating a threshold binary image </h3>\n",
    "All desribed in lection 7 Sobel transfromation has been applied one by one: \n",
    "<ul>\n",
    "<li>Sobel absolute axis x:<br/>\n",
    "<code>\n",
    "        img_gradx = sobel_abs_threshold(image, orient='x')\n",
    "</code>\n",
    "<img src=\"../output_images/02_pipeline_03.png\">\n",
    "</li>\n",
    "<li>Sobel absolute axis y:<br/>\n",
    "<code>\n",
    "        img_grady = sobel_abs_threshold(image, orient='y')\n",
    "</code>\n",
    "<img src=\"../output_images/02_pipeline_04.png\">\n",
    "</li>\n",
    "<li>Sobel magnitude:<br/>\n",
    "<code>\n",
    "        img_mag_binary = sobel_mag_threshold(image)\n",
    "</code>\n",
    "<img src=\"../output_images/02_pipeline_05.png\">\n",
    "</li>\n",
    "<li>Sobel direction<br/>\n",
    "<code>\n",
    "        img_dir_binary = sobel_dir_threshold(image)\n",
    "</code>\n",
    "<img src=\"../output_images/02_pipeline_06.png\">\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "Combine all these transformation to one image:\n",
    "<code>\n",
    "        img_combined[ (image==1) | ((img_gradx == 1) & (img_grady == 1)) | ((img_mag_binary == 1) & (img_dir_binary == 0)) ] = 1\n",
    "</code>\n",
    "<img src=\"../output_images/02_pipeline_comb.png\">\n",
    "\n",
    "<h3>4. A perspective transform</h3>\n",
    "Source and destation for perspective trasfrom depends on concrete video and calculated manually:\n",
    "<code>\n",
    "        src = np.float32( [ [ 200, 690 ],\n",
    "                        [ 640-coffs, top ],\n",
    "                        [ 640+coffs, top ],\n",
    "                        [ 1100, 690 ] ] )\n",
    "</code>\n",
    "Source:<br/>\n",
    "<code>\n",
    "            img_transformed = perspective_transform( img_combined, \n",
    "                                                top = self.pers_trans_top,\n",
    "                                                coffs = self.pers_trans_coffs,\n",
    "                                                verbose=self.verbose )\n",
    "</code>\n",
    "<img src=\"../output_images/02_pipeline_07.png\">\n",
    "Destination:<br/>\n",
    "<code>\n",
    "        dst = np.float32([[220, 720], \n",
    "                      [220, 0],\n",
    "                      [1070, 0],\n",
    "                      [1070, 720]])\n",
    "</code>\n",
    "<img src=\"../output_images/02_pipeline_08.png\">\n",
    "\n",
    "<h3>5. Identifing lane-line pixels and fit their positions with a polynomial</h3>\n",
    "In case if polunomial is not yet available lane pixels detected by sliding windows across peaks of hystogram. In other case search around poly strataegy has been applied: \n",
    "<code>\n",
    "        if len( self.left_fit ) == 0 and len( self.right_fit ) == 0 :            \n",
    "            # Find our lane pixels first\n",
    "            new_leftx, new_lefty, new_rightx, new_righty, out_img = find_lane_pixels( img_transformed )\n",
    "        else:\n",
    "            new_leftx, new_lefty, new_rightx, new_righty, out_img = self.search_around_poly( img_transformed )\n",
    "</code>\n",
    "\n",
    "Polynomials calculated base on x and y coordinates:\n",
    "<code>\n",
    "        new_left_fit = np.polyfit(new_lefty,  new_leftx,  2)\n",
    "        new_right_fit = np.polyfit(new_righty, new_rightx, 2)\n",
    "</code>\n",
    "\n",
    "Also additinal checks has been performed for detecting edges and error cases: lines 260-299\n",
    "<img src=\"../output_images/02_pipeline_09.png\">\n",
    "\n",
    "<h3>6. Calculated the radius of curvature of the position of the vehicle</h3>\n",
    "Lanes curvature and offset calculation based on code from lesson 8\n",
    "<code>\n",
    "    def calculate_curvature_and_offset(self, leftx, lefty, rightx, righty ) :\n",
    "</code>\n",
    "lines 35-67\n",
    "\n",
    "<h3>7. Result image</h3>\n",
    "\n",
    "<img src=\"../output_images/02_pipeline_10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipeline (video)</h2>\n",
    "Preprocessed video available here:\n",
    "<ul>\n",
    "<li> <a href=\"../output_videos/01_project_video.mp4\"> Project video </a> </li>\n",
    "<li> <a href=\"../output_videos/02_challenge_video.mp4\"> Challenge video </a> </li>\n",
    "<li> On harder challenge video lanes <font color = \"red\">can't be</font> detected. Plase take a <a href=\"../output_videos/03_harder_challenge_video.mp4\"> look </a> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Discussion</h2>\n",
    "\n",
    "<h3>Problems</h3>\n",
    "<ul>\n",
    "    <li>Some of parameters can be only hadrcoded and should be tuned (choosed) manually</li>\n",
    "    <li>Manual adjustment of paramets consume a lot of time</li>\n",
    "    <li>Hard to investigate pipeline errors especially when memory mechanism has been introduced.</li> \n",
    "</ul>\n",
    "<h3>Possible improvenets</h3>\n",
    "<ul>\n",
    "    <li>Implement some set of tools for autmate coosing of parameters like a RIO size, filters threshhold</li>\n",
    "    <li>More sofisticate memory and spartial filters can be impelemented</li>\n",
    "    <li>Debugging technick can be improved</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
